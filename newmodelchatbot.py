# -*- coding: utf-8 -*-
"""newmodelchatbot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13eF6i-jmZRHXlOWS6deqRCHE8o_Gtxfc
"""

pip install --user --upgrade tensorflow-model-optimization

!pip install transformers

from transformers import BertTokenizer, TFBertForSequenceClassification
from transformers import InputExample, InputFeatures

model = TFBertForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

model.summary()

import tensorflow as tf
import pandas as pd

import pandas as pd

data = pd.read_json("/content/dataeditedcompleted.json")
data

data.intents[10]

data.columns

data1 = pd.read_excel("/content/chatbot data (1).xlsx")
data1

data1.columns

df0 = data1[["words",
       'Respond ']]
df = df0.rename(columns={"words":"qu",
                  'Respond ':"answer"})  
df = df.dropna()
df

print(df.dtypes)

df = df.astype(str)
#print(df.dtypes)
df.dtypes

import numpy as np
from sklearn.preprocessing import OrdinalEncoder

enc = OrdinalEncoder()
answer_num = enc.fit_transform(np.array(df["answer"]).reshape(-1, 1))
df["answer_num"] = answer_num.astype(np.int)
df_final = df[["qu", "answer_num"]]
df_final

enc.categories_[0]

from sklearn.model_selection import train_test_split

train, test = train_test_split(df_final)

!pip install translate

num_classes = len(df_final["answer_num"].value_counts())
num_classes

import tensorflow as tf
from sklearn.model_selection import train_test_split

y = tf.keras.utils.to_categorical(df_final["answer_num"].values,
                                  num_classes=num_classes)

x_train, x_test, y_train, y_test = train_test_split(df_final['qu'], y, test_size=0.25)

!pip install tensorflow_text

import tensorflow_hub as hub
import tensorflow_text as text

preprocessor = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-preprocess/2")
encoder = hub.KerasLayer("https://tfhub.dev/google/universal-sentence-encoder-cmlm/multilingual-base/1")

from keras import backend as K

def balanced_recall(y_true, y_pred):
    """This function calculates the balanced recall metric
    recall = TP / (TP + FN)
    """
    recall_by_class = 0
    # iterate over each predicted class to get class-specific metric
    for i in range(y_pred.shape[1]):
        y_pred_class = y_pred[:, i]
        y_true_class = y_true[:, i]
        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true_class, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        recall_by_class = recall_by_class + recall
    return recall_by_class / y_pred.shape[1]

def balanced_precision(y_true, y_pred):
    """This function calculates the balanced precision metric
    precision = TP / (TP + FP)
    """
    precision_by_class = 0
    # iterate over each predicted class to get class-specific metric
    for i in range(y_pred.shape[1]):
        y_pred_class = y_pred[:, i]
        y_true_class = y_true[:, i]
        true_positives = K.sum(K.round(K.clip(y_true_class * y_pred_class, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred_class, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        precision_by_class = precision_by_class + precision
    # return average balanced metric for each class
    return precision_by_class / y_pred.shape[1]

def balanced_f1_score(y_true, y_pred):
    """This function calculates the F1 score metric"""
    precision = balanced_precision(y_true, y_pred)
    recall = balanced_recall(y_true, y_pred)
    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))

i = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')
x = preprocessor(i)
x = encoder(x)
x = tf.keras.layers.Dropout(0.2, name="dropout")(x['pooled_output'])
x = tf.keras.layers.Dense(num_classes, activation='softmax', name="output")(x)

model = tf.keras.Model(i, x)

import tensorflow as tf